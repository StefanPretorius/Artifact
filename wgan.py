# -*- coding: utf-8 -*-
"""WGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVRCfTwcaksVz5NLzcGWf2Ot2akwG6XR

# WGAN-GP - Wasserstein GAN
"""

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import os
import time
from IPython import display
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2DTranspose
tf.__version__

#Path to the dataset directory
dataset_directory = r'C:\Users\NWU-4060-2\Desktop\ITRI 671 - Artifact\Dataset\TypeII'

#Define the batch size
batch_size = 30

#Load the dataset
X_train = image_dataset_from_directory(
    dataset_directory,
    labels=None,
    image_size=(128, 128),
    color_mode='grayscale',
    batch_size=batch_size,
    shuffle=True
)


#Function to normalize and reshape images
def normalize_and_reshape(images):
    images = tf.cast(images, tf.float32)
    images = (images - 127.5) / 127.5
    images = tf.reshape(images, (-1, 128, 128, 1))
    return images


#Apply the normalization and reshaping function
train_ds = X_train.map(normalize_and_reshape)

# Check the shape of the dataset
for images in train_ds.take(1):
    print(images.shape)

"""## Loading and preprocessing the dataset"""

train_ds.element_spec

buffer_size = 870
batch_size = 30
print(type(train_ds))
print(train_ds.element_spec)

"""## Building the generator

- The same architecture of DCGAN
"""

def build_generator():
    network = tf.keras.Sequential()

    #Dense layer: 8x8x256 units
    network.add(layers.Dense(units=8*8*256, use_bias=False, input_shape=(100,)))
    network.add(layers.BatchNormalization())
    network.add(layers.LeakyReLU())

    #Reshape to 8x8x256
    network.add(layers.Reshape((8, 8, 256)))

    #Transpose Conv Layer 1: 8x8x256 -> 16x16x128
    network.add(layers.Conv2DTranspose(filters=128, kernel_size=(5, 5), padding='same', strides=(2, 2), use_bias=False))
    network.add(layers.BatchNormalization())
    network.add(layers.LeakyReLU())

    #Transpose Conv Layer 2: 16x16x128 -> 32x32x64
    network.add(layers.Conv2DTranspose(filters=64, kernel_size=(5, 5), padding='same', strides=(2, 2), use_bias=False))
    network.add(layers.BatchNormalization())
    network.add(layers.LeakyReLU())

    #Transpose Conv Layer 3: 32x32x64 -> 64x64x32
    network.add(layers.Conv2DTranspose(filters=32, kernel_size=(5, 5), padding='same', strides=(2, 2), use_bias=False))
    network.add(layers.BatchNormalization())
    network.add(layers.LeakyReLU())

    #Transpose Conv Layer 4: 64x64x32 -> 128x128x1 (Final output)
    network.add(layers.Conv2DTranspose(filters=1, kernel_size=(5, 5), padding='same', strides=(2, 2), use_bias=True, activation='tanh'))

    network.summary()

    return network

generator = build_generator()

noise = tf.random.normal([1, 100])
noise

generated_image = generator(noise, training = False)
plt.imshow(generated_image[0, :,:,0], cmap='gray');

"""## Building the discriminator

- The discriminator is also called as "critic"
"""

def build_discriminator():
  network = tf.keras.Sequential()

  #14x14x64
  network.add(layers.Conv2D(filters = 64, strides = (2,2), kernel_size = (5,5), padding = 'same', input_shape = [128,128,1]))
  network.add(layers.LeakyReLU())
  network.add(layers.Dropout(0.3))

  #7x7x128
  network.add(layers.Conv2D(filters = 128, strides = (2,2), kernel_size = (5,5), padding = 'same'))
  network.add(layers.LeakyReLU())
  network.add(layers.Dropout(0.3))

  network.add(layers.Flatten())
  network.add(layers.Dense(1))

  network.summary()

  return network

discriminator = build_discriminator()

discriminator(generated_image, training = False)

"""
## Function to calculate **Wasserstein Loss**

The (*loss*) is the difference between the expected output of the discriminator for authentic images and the expected output of the discriminator for fake images (generated by the generator).

* The objective of the discriminator is to maximize this difference, while the objective of the generator is to minimize it.
"""

def loss_generator(fake_output):
  g_loss = -1. * tf.math.reduce_mean(fake_output)
  return g_loss

def loss_discriminator(real_output, fake_output, gradient_penalty):
  c_lambda = 10
  d_loss = tf.math.reduce_mean(fake_output) - tf.math.reduce_mean(real_output) + c_lambda * gradient_penalty
  return d_loss

"""## **Gradient Penalty**

Steps to calculate gradient penalty:
1. Calculate an interpolated image from the real and fake image (`(real_image * epsilon + fake_image * (1 â€” epsilon))`)
2. Calculate the gradient of the discriminator output relative to the interpolated image. After, we calculate the gradient norm.
3. Finally, the penalty is calculated as an average of the square of (norm - 1), as we want the norm to be close to one.
"""

@tf.function
def gradient_penalty(real, fake, epsilon):
  interpolated_images = real * epsilon + fake * (1 - epsilon)
  with tf.GradientTape() as tape:
    tape.watch(interpolated_images)
    scores = discriminator(interpolated_images)[0]
  gradient = tape.gradient(scores, interpolated_images)[0]
  gradient_norm = tf.norm(gradient)
  gp = tf.math.reduce_mean((gradient_norm - 1)**2)
  return gp

generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5, beta_2 = 0.9)
discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5, beta_2 = 0.9)

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoints')

checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,
                                 discriminator_optimizer = discriminator_optimizer,
                                 generator = generator,
                                 discriminator = discriminator)

"""## Training and visualizing the results"""

epochs = 1000
noise_dim = 100
number_of_images = 16
seed = tf.random.normal([number_of_images, noise_dim])

seed

def training_step(images):
  noise = tf.random.normal([batch_size, noise_dim])
  discriminator_extra_steps =   3
  for i in range(discriminator_extra_steps):
    with tf.GradientTape() as d_tape:
      generated_images = generator(noise, training = True)
      real_output = discriminator(images, training = True)
      fake_output = discriminator(generated_images, training = True)
      epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)
      gp = gradient_penalty(images, generated_images, epsilon)

      d_loss = loss_discriminator(real_output, fake_output, gp)
    discriminator_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)
    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))

  with tf.GradientTape() as g_tape:
    generated_images = generator(noise, training = True)
    fake_output = discriminator(generated_images, training = True)
    g_loss = loss_generator(fake_output)
  generator_gradients = g_tape.gradient(g_loss, generator.trainable_variables)
  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))

def create_and_save_images(model, epoch, test_input):
  preds = model(test_input, training = False)
  fig = plt.figure(figsize = (4,4))
  for i in range(preds.shape[0]):
    plt.subplot(4,4,i+1)
    plt.imshow(preds[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
    plt.axis('off')
  plt.savefig('img_epoch_{:04d}'.format(epoch))
  plt.show()

def train(dataset, epochs):
  for epoch in range(epochs):
    initial = time.time()
    for img_batch in dataset:
      if len(img_batch) == batch_size:
        training_step(img_batch)

    create_and_save_images(generator, epoch + 1, seed)
    if (epoch + 1) % 10 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)
    print('Time taken to process epoch {} was {} seconds'.format(epoch + 1, time.time() - initial))


  create_and_save_images(generator, epochs, seed)
  generator.save('generator.h5')

train(train_ds, epochs)

checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

seed_input = tf.random.normal([number_of_images, noise_dim])
preds = generator(seed_input, training = False)
fig = plt.figure(figsize = (4,4))
for i in range(preds.shape[0]):
  plt.subplot(4,4,i+1)
  plt.imshow(preds[i, :, :, 0] * 127.5 + 127.5, cmap = 'gray')
  plt.axis('off')

import os
from tensorflow.keras.models import load_model
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

#Load the generator model
generator = load_model('generator.h5')

#Generate random noise and save images
def generate_and_save_images(generator, num_images=5, noise_dim=100, save_folder=''):
    # Create the folder if it doesn't exist
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)

    noise = np.random.normal(0, 1, (num_images, noise_dim))
    generated_images = generator.predict(noise)

    #Rescale images to 0 - 1 range for displaying
    generated_images = 0.5 * generated_images + 0.5

    #Save and display the generated images
    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))
    for i in range(num_images):
        image_array = generated_images[i].reshape(128, 128)

        #Save the image using PIL
        image = Image.fromarray((image_array * 255).astype(np.uint8))  # Convert to uint8 for saving
        image.save(f'{save_folder}/generated_image_{i+1}.png')

        #Display the image
        axes[i].imshow(image_array, cmap='gray')
        axes[i].axis('off')

    plt.show()

# all the function to generate and save 5 images
generate_and_save_images(generator, num_images=5, noise_dim=100, save_folder='Generated_Images')